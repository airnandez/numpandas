{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pandas tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://pandas.pydata.org/static/img/pandas.svg\" alt=\"pandas\" width=\"15%\" height=\"15%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "    <tr>\n",
    "    <td><a href=\"https://colab.research.google.com/github/airnandez/numpandas/blob/master/notebooks/pandas.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a></td>\n",
    "    <td><a href=\"https://mybinder.org/v2/gh/airnandez/numpandas/master?filepath=notebooks%2Fpandas.ipynb\">\n",
    "  <img src=\"https://mybinder.org/badge_logo.svg\" alt=\"Launch Binder\"/>\n",
    "</a></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Author: Fabio Hernandez*\n",
    "\n",
    "*Last updated: 2024-03-06*\n",
    "\n",
    "*Location:* https://github.com/airnandez/numpandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "## Introduction\n",
    "\n",
    "This is a short tutorial for helping you getting familiar with the **pandas** library, which is built on top of NumPy: you can find an introduction to NumPy in [this notebook](NumPy.ipynb).\n",
    "\n",
    "This tutorial draws inspiration, ideas and sometimes material from several publicly available sources. Please see the [Acknowledgements](#Acknowledgements) section for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "## Reference documentation\n",
    "\n",
    "The entry point to the documentation of the stable release of pandas is http://pandas.pydata.org/pandas-docs/stable. It includes a [user guide](http://pandas.pydata.org/pandas-docs/stable/user_guide/index.html), an [API reference](http://pandas.pydata.org/pandas-docs/stable/reference/index.html) and a [cheat sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf).\n",
    "\n",
    "The [DataCamp pandas Cheat Sheet](https://assets.datacamp.com/blog_assets/PandasPythonForDataScience.pdf) can also be a useful resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "## ⚠️ Important: installing dependencies ⚠️\n",
    "\n",
    "If you are running this notebook in Google Colab the cell below installs a recent version of Pandas. This notebook is tested against Pandas v2.2.1 and Google Colab uses v1.5.3 by default which does not support some features used by this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [[ -n ${COLAB_RELEASE_TAG} ]]; then\n",
    "  pip install --upgrade openpyxl\n",
    "  pip install pandas>=2.2.1\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "## Import\n",
    "\n",
    "**pandas** is customarily imported as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, for the examples given in this notebook we will need some packages from the Python standard library so we import them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Overview\n",
    "\n",
    "**pandas** offers three main data structures designed to facilitate the programmatic manipulation of datasets with flexibility. Those data structures are [`DataFrame`](https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#dataframe), [`Series`](https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#series) and `Index`. We will start exploring what a `DataFrame` is and what we can do with it.\n",
    "\n",
    "![dataframe](../images/dataframe-axis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "## Load the dataset\n",
    "\n",
    "Read a sample dataset, located in the `data` subdirectory, which is formatted as a sequence of lines, each line composed of series of comma-separated values. Our sample dataset contains some data about the European Union, extracted from several sources, including [Wikipedia](https://en.wikipedia.org/wiki/European_Union), [EuroStat](https://ec.europa.eu/eurostat) and the [EU Budget](http://ec.europa.eu/budget) site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def download(url: str, path: str):\n",
    "    \"\"\"Download file at url and save it locally at path.\"\"\"\n",
    "    with requests.get(url, stream=True) as resp:\n",
    "        if not resp.ok:\n",
    "            raise f'Could not find file at URL {url}'\n",
    "            \n",
    "        mode, data = 'wb', resp.content\n",
    "        if 'text/plain' in resp.headers['Content-Type']:\n",
    "            mode, data = 'wt', resp.text\n",
    "        with open(path, mode) as f:\n",
    "            f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset if necessary to the directory 'data'\n",
    "data_dir = 'data'\n",
    "path = os.path.join('..', data_dir, 'european_union-2020.csv')\n",
    "\n",
    "if not os.path.isfile(path):\n",
    "    os.makedirs(os.path.join('..', data_dir), exist_ok=True)\n",
    "    url = 'https://raw.githubusercontent.com/airnandez/numpandas/master/data/european_union-2020.csv'\n",
    "    download(url, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This particular dataset uses ';' as column separator (instead of the more usual ',')\n",
    "# and uses ',' as the decimal separator\n",
    "df = pd.read_csv(path, sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the dimensions of the dataframe\n",
    "rows, columns = df.shape\n",
    "print(f'This dataframe has {rows} rows and {columns} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pandas** has built-in methods for doing I/O with files in several formats, including flat files (csv, fixed-width format, msgpack), Excel, JSON, HTML, HDF5, parquet, SQL, etc. See the [documentation](http://pandas.pydata.org/pandas-docs/stable/reference/io.html#flat-file) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "## Exploring the dataset contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of what data is included in the dataset, you can explore the contents of the whole dataframe.\n",
    "\n",
    "⚠️ **WARNING** ⚠️: generally speaking, it is not a good idea to display the entire dataset, depending of the size of the data. It is recommended to first inspect the size of the dataframe as we did above. Our dataset is small, so we can display all of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also explore a fraction of the dataset by displaying, for instance, a few rows at the begining or at the end of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 3 rows of the dataset. By default, the first 5 rows will be displayed\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also explore the last rows of the dataset or any intermediate rows, by using notation similar to the one used with NumPy arrays, on top of which **pandas** is built:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last 3 rows of the dataset\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the rows from position 10 up to position 14 (not included)\n",
    "df[10:14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying a small random sample of the dataframe rows is generally good practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 5 randomly selected rows and all columns\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pandas** is designed for efficient handling of datasets organized as follows:\n",
    "\n",
    "* each **observation** is saved in its own row\n",
    "* each **variable** is saved in its own column\n",
    "\n",
    "Our sample dataset is organized in exactly this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An aside: understanding the dataset\n",
    "\n",
    "In order to analyse any dataset, you need first to understand the meaning of the data. Here are the details of our sample data set:\n",
    "\n",
    "| column                                    | meaning |\n",
    "| ------------------------------------------|----------|\n",
    "| `country`                                 | name of the country, in English |\n",
    "| `country_code`                            | code of the country, as used by [Eurostat](https://ec.europa.eu/eurostat/statistics-explained/index.php/Glossary:Country_codes) |\n",
    "| `accession_date`                          | date of accession of the country to the European Union (format: `yyyy-mm-dd`) |\n",
    "| `population`                              | the number of persons having their usual residence in each country as of January 1st, 2020 (source: [Eurostat](https://ec.europa.eu/eurostat/tgm/table.do?tab=table&plugin=1&language=en&pcode=tps00001)) |\n",
    "| `euro_zone_member`                        | `True` if the country is member of the [Eurozone](https://en.wikipedia.org/wiki/Eurozone)  |\n",
    "| `immigration`                             | total number of long-term immigrants arriving into the country in 2019, as reported by each country (source: [Eurostat](https://ec.europa.eu/eurostat/tgm/table.do?tab=table&plugin=1&language=en&pcode=tps00176)) |\n",
    "| `emigration`                              | total number of long-term emigrants leaving from the reporting country in 2019, as reported by each country (source: [Eurostat](https://ec.europa.eu/eurostat/tgm/table.do?tab=table&plugin=1&language=en&pcode=tps00177))  | \n",
    "| `contribution_to_eu_budget_millions_euro` | contribution to the EU budget for each country for year 2019, in millions euros (source: [European Commission](http://ec.europa.eu/budget/graphs/revenue_expediture.html)) |\n",
    "| `expenditure_eu_budget_millions_euro`     | expenditure of the EU budget per country (for all programs), for year 2019, in millions euros (source: [European Commission](http://ec.europa.eu/budget/graphs/revenue_expediture.html)) |\n",
    "\n",
    "Generally speaking, in order to draw sensible conclusions from any dataset you are analysing, make sure you understand precisely what is contained in the dataset and you understand where the data comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dataframe` properties\n",
    "\n",
    "**pandas** provides some methods for retrieving information about a [dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) object. [`pandas.Dataframe.info`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html#pandas-dataframe-info) gives summary of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of the dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the amount of memory (RAM) the dataframe is using (in bytes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the amount of bytes the dataframe is using (in RAM)\n",
    "df.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `Dataframe.columns` is an object of type `Index` (see [reference documentation](https://pandas.pydata.org/pandas-docs/stable/reference/indexing.html#index)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the list of column names in the dataframe\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns is a Python iterable (like a list)\n",
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of values (of any type) contained in the dataframe\n",
    "print(f'This dataframe contains {df.size} values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "\n",
    "Very often, the *raw* data needs some cleaning, so that we can easily manipulate them with **pandas**. For instance, in this particular example, we need to make sure that **pandas** understands that the column `accession_date` is a date and not just a string. We need this for comparisons and filtering, that will visit later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the types of each column in the dataframe\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column 'accession_date' to a date\n",
    "df['accession_date'] = df['accession_date'].astype('datetime64[s]')\n",
    "df['accession_date'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Selecting and filtering\n",
    "\n",
    "**pandas** provides powerful built-in tools for filtering the data both row-wise and column-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a utility function we use for displaying the dataframe, which we use later\n",
    "def highlight_column(s):\n",
    "    return 'background-color: PaleGoldenrod'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select all values in a given column\n",
    "\n",
    "Selecting all the values in a column is a frequent operation we need to perform on any dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight the column 'population' that we want to select\n",
    "df.head(3).style.map(highlight_column, subset=['population'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the values of the column 'population' for all rows\n",
    "df['population']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value returned by this selection operation is a [pandas.Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html) object. A `Series` is a **one-dimensional array with axis labels**. In this particular case the labels are integers but they may be of other types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: it is possible to use the notation `df.population` to select all the values of the column `\"population\"`. However, this notation is not recommended since the name of the column must be a valid Python identifier for it to work. For instance, if the name of my column is `budget-contribution`, this notation cannot be used as if would be `df.budget-contribution`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This notation is NOT recommended because it is not robust. Use instead:: df['population']\n",
    "df.population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform operations on all the numerical values of a column (i.e. a `pandas.Series` object), such as descriptive statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some descriptive statistics of the values in the 'population' column\n",
    "df['population'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also compute a subset of the descriptive statistics or perform an arithmetic operation on all the values of the `Series`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the number of values in the column 'population'\n",
    "df['population'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and standard deviation of the values in the column 'population'\n",
    "mean, std = df['population'].mean(), df['population'].std()\n",
    "print(f'Population: µ={mean:,.0f}  σ={std:,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum all the values of the column 'population'\n",
    "eu_population = df['population'].sum()\n",
    "print(f'The population of the EU in 2020 was {eu_population:,} people')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also perform an operation on all the values of one (or more) columns. For instance, let's convert all the population values to millions before performing some additional operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = df['population'] / 1_000_000\n",
    "population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide all the values of the column 'population' by one million\n",
    "population = df['population'] / 1_000_000  # you can also use the notations 1e6 or 1000000\n",
    "\n",
    "# Retrieve the min and max values of the series\n",
    "min_population, max_population = population.min(), population.max()\n",
    "\n",
    "# Sum all the values of the series\n",
    "total_population = population.sum()\n",
    "\n",
    "# Count the number of values in the series\n",
    "num_countries = population.count()\n",
    "\n",
    "print(f'The least populous country has {min_population:.1f} millions')\n",
    "print(f'The most populous country has {max_population:.1f} millions')\n",
    "print(f'Total EU population in 2020 was {total_population:.1f} millions located in {num_countries} countries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### select rows satisfying one or more conditions\n",
    "\n",
    "You can select the rows of the dataframe that satisfy one or more conditions on the values of a column. You can use logical expressions with those conditions (i.e. using boolean operators and, or, not) to select the rows of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first visualize the column 'euro_zone_member'\n",
    "df.style.map(highlight_column, subset=['euro_zone_member'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all the rows with boolean value 'True' in the column 'euro_zone_member'. This operation\n",
    "# returns a \"mask\" that we will use afterwards to select the rows. A mask is a pandas.Series object\n",
    "# which contains boolean values.\n",
    "is_eurozone_member = df['euro_zone_member'] == True\n",
    "\n",
    "is_eurozone_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the mask created above to select the rows in 'df' for which the mask is 'True'. The returned value\n",
    "# of this operation is a pandas.Dataframe which is a view of the original dataframe 'df'\n",
    "euro_zone_df = df[is_eurozone_member]\n",
    "\n",
    "# Note that the dataframe 'euro_zone_df' only contains rows which value in the column 'euro_zone_member' is 'True'\n",
    "euro_zone_df.style.map(highlight_column, subset=['euro_zone_member'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is another more compact way of expressing the same filter although is less readable\n",
    "euro_zone_df = df[df['euro_zone_member'] == True]\n",
    "euro_zone_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this kind of selection operation is generally a dataframe object. You can perform operations on that dataframe as you would on any other dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the population of the eurozone, in millions\n",
    "is_eurozone_member = df['euro_zone_member'] == True\n",
    "eurozone_population = df[is_eurozone_member]['population'].sum() / 1_000_000\n",
    "\n",
    "print(f'The population of the Euro zone in 2020 was {eurozone_population:.2f} millions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also select the rows of a dataframe that satisfy *several conditions*, by combining several masks using boolean operations (and, or, not, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the countries of the Euro zone, which joined the EU since year 1989\n",
    "is_eurozone_member = df['euro_zone_member'] == True\n",
    "joined_since_1989  = df['accession_date'] >= datetime.datetime(1989, 1, 1)\n",
    "\n",
    "# Combine the two masks obtained above with an 'and' (&) operator\n",
    "df[is_eurozone_member & joined_since_1989]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the rows for countries which are either EU founder members or have a population\n",
    "# of at least 20M people. EU founder members are those which joined the date of the\n",
    "# foundation of the EU, that is 1957-03-25.\n",
    "is_founder         = df['accession_date'] == datetime.datetime(1957, 3, 25)\n",
    "is_bigger_than_20m = df['population'] >= 20_000_000\n",
    "\n",
    "# Combine the two masks obtained above with an \"or\" (|) operator\n",
    "df[is_founder | is_bigger_than_20m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the populations of EU founder members vs. non-founder member countries\n",
    "# Founder members are those which 'accession_date' is 1957-03-25\n",
    "is_founder = df['accession_date'] == datetime.datetime(1957, 3, 25)\n",
    "\n",
    "founders_population     = df[ is_founder]['population'].sum() / 1e6\n",
    "non_founders_population = df[~is_founder]['population'].sum() / 1e6  # Note the notation '~' which is the logical NOT\n",
    "\n",
    "print(f\"Founder countries population:     {founders_population:.0f} millions\")\n",
    "print(f\"Non-founder countries population: {non_founders_population:.0f} millions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select specific rows\n",
    "\n",
    "One of the most useful methods for selecting rows and columns within a row is [DataFrame.loc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html). It accepts several forms as input, but a general one is:\n",
    "\n",
    "`df.loc[rows, cols]`\n",
    "\n",
    "where `rows` and `cols` are slices (e.g. `10:15`, `10:`, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the values of all the columns for row with index 15\n",
    "df.loc[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the rows with indices in the interval [10,15]\n",
    "df.loc[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the value of the column 'population' for the row with index 11\n",
    "df.loc[11, 'population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all the columns of the row for France\n",
    "# We need to provide the index of the row we want to retrieve, 11 in this particular case and\n",
    "# the interval of the columns of interest (all, in this particular case)\n",
    "df.loc[11, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve specific columns of a given row\n",
    "df.loc[11, ['capital', 'contribution_to_eu_budget_millions_euro']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve specific columns of a range of rows\n",
    "df.loc[5:11, ['country', 'population']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set a meaningful index\n",
    "\n",
    "It is convenient to use an index which allows us to select an entire row or specific columns within a row using a meaningful label. You can set the index of a dataframe when you load it from a file or after the dataframe is already in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of our dataframe, the index for each row is automatically assigned by pandas as an integer (leftmost column in the table above). For convenience, we can modify that label to use instead the country code as the index of the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the contents of the `country_code` column as the dataframe index\n",
    "# We don't want the original dataframe to be modified, so we use a new variable\n",
    "df_new = df.set_index('country_code')\n",
    "\n",
    "df_new.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use that more meaningful index to select the rows of interest for our analysis, without actually needing to know their row numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve specific columns of a given row using the country code which is\n",
    "# now the dataframe index\n",
    "df_new.loc['FR', ['capital', 'population']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the populations for countries ES and DE\n",
    "df_new.loc[['ES', 'DE'], ['population']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set the index when loading the data to memory, by specifying the column number we want to use as the index of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and set the dataframe index to the first column which contains the\n",
    "# country code, instead of the default row number\n",
    "df = pd.read_csv('../data/european_union-2020.csv', sep=';', decimal=',', index_col=1)\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "It is possible to work with a projection of the dataframe by filtering the rows or columns we need to act on (e.g. query, modify, etc):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns 'country' and 'capital' on all the rows of the dataset\n",
    "df.filter(items=['country', 'capital']).sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also select rows or columns which match a regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the rows with country code ending by 'E'. Note that 'filter' method acts on the index of the dataframe\n",
    "# or on the names of the columns, not on their values\n",
    "df.filter(regex='.E$', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive the rows for countries which value in the column 'capital' starts by 'B' and ends by 't'\n",
    "df[ df['capital'].str.contains('^B.+t$', regex=True) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## Sorting\n",
    "\n",
    "You can sort the contents of a dataframe, according to the values of a set of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mediterraneans = ('Spain', 'France', 'Italy', 'Slovenia', 'Croatia', 'Greece')\n",
    "is_mediterranean = df['country'].isin(mediterraneans)\n",
    "is_mediterranean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the rows of the mediterranean countries\n",
    "mediterraneans = ('Spain', 'France', 'Italy', 'Slovenia', 'Croatia', 'Greece')\n",
    "is_mediterranean = df['country'].isin(mediterraneans)\n",
    "\n",
    "# Sort the selected rows according to the values of columns 'accession_date' and then 'population'\n",
    "df[is_mediterranean].sort_values(by=['accession_date', 'population'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also retrieve the N largest (or N smallest) rows, according to the value of some columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 5 countries according to the values in the 'emigration' column\n",
    "df.nlargest(5, columns=['emigration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 3 less populous countries\n",
    "df.nsmallest(3, columns=['population'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "## Plotting\n",
    "\n",
    "When exploring a dataset visualizing a projection of its contents is often useful. **pandas** provides some built-in tools for quick visualisations, based on [matplotlib](https://matplotlib.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the population of the countries, in descending order\n",
    "populations = df['population'].sort_values(ascending=False)\n",
    "populations.plot.bar(figsize=(15,8))    # figure size in inches: 1 inch ≃ 2.5 cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to improve the plots, such as adding a title for the figure and modifying the axes labels. You can refer to the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html#pandas.DataFrame.plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the population of EU countries (in millions)\n",
    "populations = df['population'] / 1e6\n",
    "figure = populations.plot.hist(figsize=(15,8), title=\"Distribution of the population of EU countries (2020)\", grid=True)\n",
    "figure.set_xlabel(\"millions\")\n",
    "figure.set_ylabel(\"countries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "## Serializing a dataframe\n",
    "\n",
    "You can save the contents of a dataframe to a disk file. **pandas** natively support several formats (see [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#serialization-io-conversion)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset in 'parquet' format\n",
    "parquet_path = os.path.join('..', 'data', 'european_union-2020.parquet')\n",
    "df.to_parquet(parquet_path, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that now we have a 'parquet' file in our 'data' directory\n",
    "import glob\n",
    "\n",
    "glob.glob('../data/european_union-2020.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -al ../data/european_union-2020.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back the dataset from the parquet file just created\n",
    "new_df = pd.read_parquet(parquet_path)\n",
    "new_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "## Modifying the dataframe\n",
    "\n",
    "You will often need to modify the dataframe, for instance, for cleaning it, for extending it or for computing new values useful in the data analysis process.\n",
    "\n",
    "Please note that the modifications are applied to the in-memory data, not to the disk file, unless you explicitely save the dataframe to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename some dataframe columns to use shorter, more convenient names\n",
    "df = df.rename(columns={\n",
    "    # current column name                      new column name\n",
    "    'contribution_to_eu_budget_millions_euro': 'budget_contribution',\n",
    "    'expenditure_eu_budget_millions_euro':     'budget_expenditure',\n",
    "})\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also **extend the dataframe** by creating new columns, which values may be computed using other columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add two new columns 'budget_contribution_per_capita' and 'budget_expenditure_per_capita' to store the computed\n",
    "# budget contribution and budget expenditure per capita.\n",
    "# Note that the budget figures in the dataset are in millions\n",
    "df['budget_contribution_per_capita'] = (df['budget_contribution'] * 1_000_000 ) / df['population']\n",
    "df['budget_expenditure_per_capita']  = (df['budget_expenditure']  * 1_000_000 ) / df['population']\n",
    "\n",
    "df.head(4).style.map(highlight_column, subset=['budget_contribution_per_capita', 'budget_expenditure_per_capita'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute a `panda.Series` of values from the values in the columns of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the net per-capita contribution to the EU budget for each country\n",
    "net_contribution_per_capita = df['budget_contribution_per_capita'] - df['budget_expenditure_per_capita']\n",
    "net_contribution_per_capita.sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The net contribution by France to the 2019 EU budget was approx. {net_contribution_per_capita[\"FR\"]:.0f}€ per capita')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the methods `idxmin()` (or `idxmax()`) to retrieve the **index of the row** which contains the minimum (or maximum) value of a column in a `panda.Series`, as opposed to the minium (or maximum) value itself. In our case, we can use this to retrieve the country code (i.e. the value of the dataframe index) and then the country name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive the indexes of the rows with minimum and maximum values in the series 'net_contribution_per_capita'\n",
    "net_contribution_per_capita.idxmin(), net_contribution_per_capita.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the value of the minimum net contribution per capita\n",
    "net_contribution_per_capita.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_min, index_max = net_contribution_per_capita.idxmin(), net_contribution_per_capita.idxmax()\n",
    "df.loc[[index_min, index_max]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the name of the countries for those minimum and maximums\n",
    "df.loc[[index_min, index_max], 'country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the code of the countries with minimum and maximum value on the\n",
    "# 'net_contribution_per_capita' series (computed above)\n",
    "country_min_expenditure = df.loc[net_contribution_per_capita.idxmin(), 'country']\n",
    "value_min_expenditure = net_contribution_per_capita.min()\n",
    "\n",
    "country_max_expenditure = df.loc[net_contribution_per_capita.idxmax(), 'country']\n",
    "value_max_expenditure = net_contribution_per_capita.max()\n",
    "\n",
    "print(f'The country with lowest EU budget expenditure per capita in 2019 was:  {country_min_expenditure:>10} ({value_min_expenditure:,.0f} €)')\n",
    "print(f'The country with highest EU budget expenditure per capita in 2019 was: {country_max_expenditure:>10} ({value_max_expenditure:,.0f} €)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Grouping\n",
    "\n",
    "In some datasets, data is organized so that **grouping the observations** (i.e. the rows) is necessary to answer some analysis questions. **pandas** provides useful tools for grouping rows based on the values of one or more columns.\n",
    "\n",
    "The data in the dataset we have been working on does not require grouping. We load a different, more complex and bigger dataset to explore how grouping works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load another dataset\n",
    "\n",
    "The dataset we will use contains data about the names given to babies in France during from year 1900 to year 2021. For each given name you can find the sex of the baby (male or female), the year of birth, the department and the number of babies registered with that given name per year and per department.\n",
    "\n",
    "You can find details of this public dataset, including the exact meaning of each variable (in French),  at https://www.insee.fr/fr/statistiques/2540004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset if necessary\n",
    "data_dir = 'data'\n",
    "path = os.path.join('..', data_dir, 'prenoms-fr-1900-2021.zip')\n",
    "\n",
    "if not os.path.isfile(path):\n",
    "    os.makedirs(os.path.join('..', data_dir), exist_ok=True)\n",
    "    url = 'https://www.insee.fr/fr/statistiques/fichier/2540004/dpt2021_csv.zip'\n",
    "    download(url, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load another dataset. Its fields are separated by ';'.\n",
    "# We ask pandas to interpret the columns 'annais' and 'dpt' as strings to avoid error with missing\n",
    "# values\n",
    "names_df = pd.read_csv(path, sep=';', dtype={'annais':str, 'dpt':str})\n",
    "rows, cols = names_df.shape\n",
    "print(f'This dataset contains {rows:,} rows and {cols} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df.sample(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can find an edited excerpt of the [meaning and coding conventions of the columns](https://www.insee.fr/fr/statistiques/2540004#dictionnaire) of this dataset. You may also want to read the excellent [documentation associated to this dataset](https://www.insee.fr/fr/statistiques/2540004#documentation):\n",
    "\n",
    "*Le second fichier départemental comporte 3.784.673  enregistrements et cinq variables décrites ci-après.*\n",
    "*Ce fichier est trié selon les variables `SEXE`, `PREUSUEL`, `ANNAIS`, `DPT`.*\n",
    "\n",
    "* `SEXE`: sexe - Type : caractère - Longueur : 1 - Modalité : 1 pour masculin, 2 pour féminin\n",
    "* `PREUSUEL`: premier prénom - Type : caractère - Longueur : 25\n",
    "* `ANNAIS`: année de naissance - Type : caractère - Longueur : 4 - Modalité : 1900 à 2021, XXXX\n",
    "* `DPT`: département de naissance - Type : caractère - Longueur : 3 - Modalité : liste des départements, XX\n",
    "* `NOMBRE`: fréquence - Type : numérique - Longueur : 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the types of the columns dataset\n",
    "names_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a utility function we use for displaying the dataframe\n",
    "def highlight_missing(s):\n",
    "    missings = ('XX', 'XXXX', '_PRENOMS_RARES')\n",
    "    return 'color: white; background-color: Crimson' if s in missings else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename some columns to use more meaningful names\n",
    "names_df = names_df.rename(columns={\n",
    "    'sexe':      'sex',\n",
    "    'preusuel':  'name',\n",
    "    'annais':    'year',\n",
    "    'dpt':       'department',\n",
    "    'nombre':    'count'})\n",
    "\n",
    "names_df.head().style.map(highlight_missing, subset=['name', 'year', 'department'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are rows with missing values, which in this case are represented by the strings `XXXX` for year or `XX` for the department or `_PRENOMS_RARES` for the name column. For the purposes of this tutorial, we ignore those rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing department and year\n",
    "names_df.drop(names_df[names_df['department'] == 'XX'].index, inplace=True)\n",
    "names_df.drop(names_df[names_df['year'] == 'XXXX'].index, inplace=True)\n",
    "\n",
    "# Convert column 'year' to numeric values\n",
    "names_df['year'] = pd.to_numeric(names_df['year'])\n",
    "\n",
    "names_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, some given names are coded as `_PRENOMS_RARES`, to represent a group of given names used very few times (see the dataset documentation for details). For this exercise, we are not interested in that data, so we remove the rows in the dataset which contain that given name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_unusual = names_df['name'] == '_PRENOMS_RARES'\n",
    "names_df[is_unusual].head().style.map(highlight_missing, subset=['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the fraction of unusual names we will exclude from our analysis\n",
    "rows, _ = names_df.shape\n",
    "unusual_rows, _ = names_df[is_unusual].shape\n",
    "print(f'There are {unusual_rows:,} out of {rows:,} rows with unusual names, that is {100*unusual_rows/rows:0.2}% of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df.drop(names_df[is_unusual].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that there are no rows which contain `_PRENOMS_RARES` in the column `name`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df[names_df['name'] == '_PRENOMS_RARES'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore the dataset\n",
    "\n",
    "In this dataset, the column `sex` is coded as `1` (one) for males and `2` (two) for females. Create two views of the dataset, one for boys and one for girls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For convenience, create two views of the dataset\n",
    "boys  = names_df[names_df['sex'] == 1]\n",
    "girls = names_df[names_df['sex'] == 2]\n",
    "\n",
    "# Count the number of babies of each sex contained in the dataset\n",
    "print(f\"Babies registered from 1900 to 2021:\")\n",
    "print(f\"   boys: {boys['count'].sum():,}\")\n",
    "print(f\"  girls: {girls['count'].sum():,}\")\n",
    "print(f\"  total: {names_df['count'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aside: configure matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (8,4)\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 150\n",
    "matplotlib.rcParams[\"font.size\"] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grouping rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to plot the evolution of the babies over time. We group the rows by the value in the column `year` and for each resulting group we sum the values of the `count` column to obtain the total number of babies registered each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babies_per_year = names_df.groupby(['year'])['count'].sum()\n",
    "babies_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = babies_per_year.plot.line(title=\"Evolution of the number of registered babies\", grid=True)\n",
    "fig.set_ylabel(\"registered babies\")\n",
    "fig.set_ylim(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to focus on a subset of the rows. For instance, zoom in on the data over the period 1910 to 1925:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_ww = babies_per_year.loc[1910:1925].plot.line(title=\"Babies registered around first world war\", grid=True)\n",
    "first_ww.set_ylabel(\"registered babies\")\n",
    "first_ww.set_ylim(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to know how many boys were given the name **Zinedine** before an after year 1998, when France won the football world cup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zinedines = boys[boys['name'] == 'ZINEDINE']\n",
    "zinedines_before_1998 = zinedines[zinedines['year'] <  1998]['count'].sum()\n",
    "zinedines_after_1998  = zinedines[zinedines['year'] >= 1998]['count'].sum()\n",
    "\n",
    "print(f\"Number of boys named 'Zinedine' in France:\")\n",
    "print(f\"   before 1998: {zinedines_before_1998: 5}\")\n",
    "print(f\"    since 1998:  {zinedines_after_1998: 5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to get more details about the years those babies were named **Zinedine**, so we group the data by year: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the \"zinedines\" per year and sum the values of column 'count' for each year\n",
    "zinedines_per_year = zinedines.groupby(['year'])['count'].sum()\n",
    "zinedines_per_year.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot to visually explore the results of the operation above. For this we use **matplotlib**. Please ignore for now the details of how to use matplotlib. We look in more detail some aspects of data visualisation in [this notebook](visualisation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of \"zinedines\" as a function of the year\n",
    "zinedines_per_year.plot.bar(title=\"Evolution of number Zinedines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Joining dataframes\n",
    "\n",
    "It is usual that for analysing a dataset we need to join information found in several distinct datasets. **pandas** provides mechanisms for joining dataframes.\n",
    "\n",
    "Motivating example: we want to identify the top 5 departments where the boys named *Zinedine* were born in year 1998."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the boys, named 'ZINEDINE', born in year 1998\n",
    "zinedines_1998 = boys[(boys['name'] == 'ZINEDINE') & (boys['year'] == 1998)]\n",
    "zinedines_1998.nlargest(5, 'count').loc[:, ['department', 'count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we have in our example dataframe does not include the name of the department associated to those values (e.g. 13, 59, 69, etc.). We will use an additional helper dataframe for retrieving the names of those departments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset if necessary\n",
    "data_dir = 'data'\n",
    "path = os.path.join('..', data_dir, 'departements-region.csv')\n",
    "\n",
    "if not os.path.isfile(path):\n",
    "    os.makedirs(os.path.join('..', data_dir), exist_ok=True)\n",
    "    url = 'https://www.data.gouv.fr/en/datasets/r/987227fb-dcb2-429e-96af-8979f97c9c84'\n",
    "    download(url, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_df = pd.read_csv(path, index_col=0)\n",
    "rows, cols = dept_df.shape\n",
    "print(f'This dataset contains {rows:,} rows and {cols} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder: select the Zinedines born in 1998\n",
    "zinedines_1998 = boys[(boys['name'] == 'ZINEDINE') & (boys['year'] == 1998)]\n",
    "zinedines_1998.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can **join** both dataframes to include all the data we need in each row, in particular the name of the department:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join both datasets using the index of 'dept_df' and the column 'department' of the 'zinedines_1998' dataframe\n",
    "zinedines_1998 = boys[(boys['name'] == 'ZINEDINE') & (boys['year'] == 1998)]\n",
    "\n",
    "# Create a new dataframe which is the result of joining the dataframes 'zinedines_1998' and 'dept_df' using\n",
    "# the values in the column 'department' of the first dataframe.\n",
    "zinedines_1998_full = zinedines_1998.join(dept_df, on='department')\n",
    "\n",
    "zinedines_1998_full.sample(5).style.map(highlight_column, subset=['department', 'dep_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a dataframe with contains the selected rows each with all the information we need to answer the question: *what are the names of the top 5 deparments where the boys born in 1998 were named 'Zinedine'?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_depts = zinedines_1998_full.nlargest(5, 'count')\n",
    "top_depts[['count', 'dep_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the values of the series\n",
    "for count, dept in zip(top_depts['count'].values, top_depts['dep_name'].values):\n",
    "    print(f'{count}  {dept}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Acknowledgements\n",
    "<a id='Acknowledgements'></a>\n",
    "\n",
    "These are the sources this notebook is based on. You are encouraged to consult them to deep further:\n",
    "\n",
    "* [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) by Jave VanderPlas (highly recommended book)\n",
    "* [Intro to pandas](https://pandas.pydata.org/docs/getting_started/index.html#intro-to-pandas)\n",
    "* Data School [Pandas best practices](https://youtu.be/hl-TGI4550M) (video)\n",
    "* Dunder Data's [Intro to Pandas](https://youtu.be/31wa8tmrkPU) video series\n",
    "* Python Bootcamp organised by the [Berkeley Institute for Data Science (BIDS)](https://bids.berkeley.edu) in the Fall 2016: [videos](https://bids.berkeley.edu/news/python-boot-camp-fall-2016-training-videos-available-online) and [notebooks](https://github.com/profjsb/python-bootcamp)\n",
    "* [Python for Data Analysis](https://www.oreilly.com/library/view/python-for-data/9781491957653) 2nd Edition, by Wes McKinney"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
